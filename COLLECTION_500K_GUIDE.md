# Collecting 500,000 Genomes - Complete Guide

## Overview

This guide explains how to collect 500,000 viral genomes (250,000 viable + 250,000 non-viable) for training the VLab viability prediction model.

## Quick Start

### Simple Command (Recommended)

```bash
python3 collect_training_data.py --total_target 500000
```

This will automatically:
- Collect 250,000 viable genomes from NCBI
- Generate 175,000 synthetic non-viable genomes
- Generate 75,000 mutated non-viable genomes
- Total: 500,000 genomes (50% viable, 50% non-viable)

### With Custom Distribution

```bash
python3 collect_training_data.py \
    --max_viable 250000 \
    --num_synthetic 175000 \
    --num_mutated 75000 \
    --total_target 500000
```

## What Gets Collected

### Viable Genomes (250,000)
- **Source**: NCBI nucleotide database
- **Types**: 
  - Complete viral genomes
  - Reference genomes
  - Representative genomes
  - RefSeq genomes (highest quality)
- **Criteria**: Successfully synthesized or naturally existing viruses
- **Time**: ~20-40 hours (with API key)

### Non-Viable Genomes (250,000)

#### Synthetic Non-Viable (175,000)
Generated using multiple methods:
- **Random**: Completely random sequences
- **Fragmented**: Missing essential parts
- **No Start**: No start codons (can't initiate translation)
- **No Stop**: No stop codons (can't terminate translation)
- **Invalid Codons**: Contains N's and invalid bases
- **Too Short**: Too short to be functional
- **Missing Genes**: Fragment-like sequences

#### Mutated Non-Viable (75,000)
Generated by mutating viable genomes:
- **Delete Start**: Remove all start codons
- **Insert Stop**: Insert premature stop codons
- **Frame Shift**: Insert/delete bases to shift reading frame
- **Corrupt**: Randomly corrupt sequence

**Time**: ~1-2 hours (fast generation)

## Expected Timeline

### Total Time: ~24-48 hours

- **Viable genomes (250k)**: ~20-40 hours
  - With API key: ~20-30 hours
  - Without API key: ~40-60 hours
- **Synthetic non-viable (175k)**: ~30-60 minutes
- **Mutated non-viable (75k)**: ~1-2 hours
- **Total**: ~24-48 hours

## Resource Requirements

### Disk Space
- **Viable genomes**: ~250-500 GB
- **Non-viable genomes**: ~250-500 GB
- **Total**: ~500 GB - 1 TB

### Memory
- **RAM**: 4-8 GB recommended
- **CPU**: Multi-core recommended

### Network
- **Bandwidth**: High-speed connection recommended
- **API Key**: Required for efficient collection

## Running the Collection

### Option 1: Simple Start

```bash
# Uses your API key and email by default
python3 collect_training_data.py --total_target 500000
```

### Option 2: Background Process

```bash
# Run in background (Linux/Mac)
nohup python3 collect_training_data.py --total_target 500000 > collection.log 2>&1 &

# Or use the provided script
./start_large_collection.sh
```

### Option 3: Monitor Progress

```bash
# In one terminal - run collection
python3 collect_training_data.py --total_target 500000 --verbose

# In another terminal - monitor progress
tail -f logs/data_collection.log
watch -n 60 'find data/training -name "*.fasta" | wc -l'
```

## Checkpointing & Resumption

The system automatically:
- **Saves progress** every 1,000 sequences
- **Resumes** from last checkpoint if interrupted
- **Skips** already downloaded genomes

### Manual Resume

If interrupted, simply rerun:

```bash
python3 collect_training_data.py --total_target 500000
```

It will automatically resume from where it left off.

## Monitoring Progress

### Check Statistics

```python
from src.vlab.data.collector import DataCollector

collector = DataCollector()
stats = collector.get_data_statistics()
print(f"Viable: {stats['train_viable']:,} train, {stats['val_viable']:,} val")
print(f"Non-viable: {stats['train_non_viable']:,} train, {stats['val_non_viable']:,} val")
print(f"Total: {stats['total']:,}")
```

### Check Files

```bash
# Count genomes
find data/training -name "*.fasta" | wc -l

# Check sizes
du -sh data/training/*

# Check specific directories
ls -lh data/training/train/viable/ | head -20
```

## Data Distribution

### Final Distribution

```
Total: 500,000 genomes

Training (90%):
  Viable: 225,000
  Non-viable: 225,000
  Total: 450,000

Validation (10%):
  Viable: 25,000
  Non-viable: 25,000
  Total: 50,000
```

### Balance

- **50% Viable**: Successfully synthesized/naturally existing
- **50% Non-Viable**: Not able to be synthesized
- **Perfect balance** for training viability prediction model

## Troubleshooting

### Rate Limiting

If you hit NCBI rate limits:
- **Wait**: System will retry automatically
- **Resume**: Checkpointing handles interruptions
- **API Key**: Already configured (should help)

### Disk Space

If running out of space:
- **Monitor**: Check `du -sh data/training/`
- **Clean**: Remove old checkpoints if needed
- **Expand**: Ensure 1 TB+ available

### Memory Issues

If memory problems:
- **Reduce batch size**: Edit `max_concurrent` in code
- **Restart**: System handles resumption

### Network Issues

If network problems:
- **Resume**: System will retry and resume
- **Check connection**: Ensure stable internet
- **API Key**: Already configured for stability

## After Collection

### Step 1: Verify Data

```bash
# Check counts
find data/training -name "*.fasta" | wc -l

# Should show ~500,000 files
```

### Step 2: Train Model

```bash
python3 src/vlab/training/viability_trainer.py \
    --data_dir data/training \
    --epochs 100 \
    --batch_size 64
```

### Step 3: Use Model

Update `config.yaml`:

```yaml
viability_model_path: models/viability_model_best.pth
```

## Tips for Success

1. **Let it run**: Collection takes 24-48 hours - be patient
2. **Monitor disk**: Ensure 1 TB+ free space
3. **Check logs**: Monitor `logs/data_collection.log`
4. **Don't interrupt**: Let checkpointing handle it
5. **Verify API key**: Already configured for you

## Expected Results

After completion, you'll have:

- **500,000 total genomes**
- **250,000 viable** (from NCBI)
- **250,000 non-viable** (generated)
- **Balanced dataset** (50/50 split)
- **Train/val split** (90/10)
- **Ready for training** high-accuracy viability model

## Next Steps

1. **Wait for collection** (24-48 hours)
2. **Train model** (see training guide)
3. **Validate accuracy** (should achieve >95% with this dataset)
4. **Use in VLab** for genome analysis

## Support

- **Logs**: `logs/data_collection.log`
- **Checkpoint**: `data/training/harvest_checkpoint.json`
- **Statistics**: Use `DataCollector.get_data_statistics()`

Good luck with your collection! ðŸš€

